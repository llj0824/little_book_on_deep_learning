--- Page 117 ---
6
Chapter
Prediction
A first category of applications, such as face
recognition, sentiment analysis, object detection,
or speech recognition, requires predicting an un-
known value from an available signal.
117

--- Page 118 ---
6.1 Image denoising
A direct application of deep models to image
processing is to recover from degradation by
utilizing the redundancy in the statistical struc-
ture of images. The petals of a sunflower in a
grayscale picture can be colored with high confi-
dence, and the texture of a geometric shape such
as a table on a low-light, grainy picture can be
corrected by averaging it over a large area likely
to be uniform.
A denoising autoencoder is a model that takes
a degraded signal ˜ as input and computes an
X
estimate of the original signal . For images, it
X
is a convolutional network that may integrate
skip-connections, in particular to combine repre-
sentations at the same resolution obtained early
and late in the model, as well as attention layers
to facilitate taking into account elements that
are far away from each other.
Suchamodelistrainedbycollectingalargenum-
ber of clean samples paired with their degraded
inputs. The latter can be captured in degraded
conditions, such as low-light or inadequate fo-
cus, or generated algorithmically, for instance,
by converting the clean sample to grayscale, re-
ducing its size, or aggressively compressing it
118

--- Page 119 ---
with a lossy compression method.
The standard training procedure for denoising
autoencoders uses the MSE loss summed across
all pixels, in which case the model aims at com-
puting the best average clean picture, given the
degraded one, that is E ˜ . This quantity
[X | X]
may be problematic when is not completely
X
determined by ˜, in which case some parts
X
of the generated signal may be an unrealistic,
blurry average.
119

--- Page 120 ---
6.2 Image classification
Image classification is the simplest strategy for
extracting semantics from an image and consists
of predicting a class from a finite, predefined
number of classes, given an input image.
The standard models for this task are convolu-
tional networks, such as ResNets (see § 5.2), and
attention-based models such as ViT (see § 5.3).
These models generate a vector of logits with as
many dimensions as there are classes.
The training procedure simply minimizes the
cross-entropy loss (see § 3.1). Usually, perfor-
mance can be improved with data augmenta-
tion, which consists of modifying the training
samples with hand-designed random transfor-
mations that do not change the semantic content
of the image, such as cropping, scaling, mirror-
ing, or color changes.
120

--- Page 121 ---
6.3 Object detection
A more complex task for image understanding is
object detection, in which the objective is, given
an input image, to predict the classes and posi-
tions of objects of interest.
An object position is formalized as the four co-
ordinates of a rectangular bound-
(x ,y ,x ,y )
1 1 2 2
ing box, and the ground truth associated with
each training image is a list of such bounding
boxes, each labeled with the class of the object
contained therein.
The standard approach to solve this task, for in-
stance, by the Single Shot Detector (SSD) [Liu
et al., 2015]), is to use a convolutional neural
network that produces a sequence of image
representations of size
Z D ×H ×W , s =
s s s s
, with decreasing spatial resolution
1,...,S H ×
s
downto for (seeFigure6.1). Each
W 1×1 s = S
s
of these tensors covers the input image in full, so
the indices correspond to a partitioning of
h,w
the image lattice into regular squares that gets
coarser when increases.
s
As seen in § 4.2, and illustrated in Figure 4.4,
due to the succession of convolutional layers, a
feature vector
(Z [0,h,w],...,Z [D −1,h,w])
s s s
is a descriptor of an area of the image, called its
121

--- Page 122 ---
X
Z
1
Z
2
Z
S−1 Z
S
...
...
Figure6.1:
Aconvolutionalobjectdetectorprocessesthe
input image to generate a sequence of representations
of decreasing resolutions. It computes for every h,w, at
every scale s, a pre-defined number of bounding boxes
whose centers are in the image area corresponding to
that cell, and whose sizes are such that they fit in its
receptive field. Each prediction takes the form of the
estimates (xˆ ,xˆ ,yˆ ,yˆ ), represented by the red boxes
1 2 1 2
above, and a vector of C+1 logits for the C classes of
interest, and an additional “no object” class.
122

--- Page 123 ---
Figure6.2:
ExamplesofobjectdetectionwiththeSingle-
Shot Detector [Liu et al., 2015].
123

--- Page 124 ---
receptive field, that is larger than this square but
centered on it. This results in a non-ambiguous
matching of any bounding box to
(x ,x ,y ,y )
1 2 1 2
a , determined respectively by
s,h,w max(x −
2
, y +y , and x +x .
x ,y −y ) 1 2 1 2
1 2 1
2 2
Detection is achieved by adding convolutional
S
layers, each processing a and computing, for
Z
s
every tensor indices , the coordinates of a
h,w
bounding box and the associated logits. If there
are object classes, there are logits, the
C C+1
additional one standing for “no object.” Hence,
each additional convolution layer has
4+C+1
output channels. The SSD algorithm in particu-
lar generates several bounding boxes per ,
s,h,w
each dedicated to a hard-coded range of aspect
ratios.
Training sets for object detection are costly to
create, since the labeling with bounding boxes
requires a slow human intervention. To mitigate
this issue, the standard approach is to fine-tune
a convolutional model that has been pre-trained
on a large classification dataset such as VGG-16
for the original SSD, and to replace its final fully-
connected layers with additional convolutional
ones. Surprisingly, models trained for classifica-
tion only learn feature representations that can
be repurposed for object detection, even though
124

--- Page 125 ---
that task involves the regression of geometric
quantities.
During training, every ground-truth bounding
box is associated with its , and induces a
s,h,w
loss term composed of a cross-entropy loss for
the logits, and a regression loss such as MSE
for the bounding box coordinates. Every other
free of bounding-box match induces a
s,h,w
cross-entropy only penalty to predict the class
“no object”.
125

--- Page 126 ---
6.4 Semantic segmentation
The finest-grain prediction task for image under-
standing is semantic segmentation, which con-
sists of predicting, for each pixel, the class of the
object to which it belongs. This can be achieved
with a standard convolutional neural network
that outputs a convolutional map with as many
channels as classes, carrying the estimated logits
for every pixel.
While a standard residual network, for instance,
can generate a dense output of the same reso-
lution as its input, as for object detection, this
task requires operating at multiple scales. This
is necessary so that any object, or sufficiently
informative sub-part, regardless of its size, is
captured somewhere in the model by the feature
representation at a single tensor position. Hence,
standard architectures for this task downscale
the image with a series of convolutional layers
to increase the receptive field of the activations,
and re-upscale it with a series oftransposed con-
volutional layers, or other upscaling methods
such as bilinear interpolation, to make the pre-
diction at high resolution.
However, a strict downscaling-upscaling archi-
tecture does not allow for operating at a fine
126

--- Page 127 ---
Figure 6.3:
Semantic segmentation results with the
Pyramid Scene Parsing Network [Zhao et al., 2016].
grain when making the final prediction, since all
the signal has been transmitted through a low-
resolution representation at some point. Models
that apply such downscaling-upscaling serially
mitigate these issues withskip connections from
layers at a certain resolution, before downscal-
ing, to layers at the same resolution, after upscal-
ing [Long et al., 2014; Ronneberger et al., 2015].
Modelsthatdoitinparallel, afteraconvolutional
127

--- Page 128 ---
backbone, concatenate the resulting multi-scale
representation after upscaling, before making
the final per-pixel prediction [Zhao et al., 2016].
Training is achieved with a standard cross-
entropy summed over all the pixels. As for ob-
ject detection, training can start from a network
pre-trained on a large-scale image classification
dataset to compensate for the limited availability
of segmentation ground truth.
128

--- Page 129 ---
6.5 Speech recognition
Speech recognition consists of converting a
sound sample into a sequence of words. There
have been plenty of approaches to this problem
historically, but a conceptually simple and recent
one proposed by Radford et al. [2022] consists of
casting it as a sequence-to-sequence translation
and then solving it with a standard attention-
based Transformer, as described in § 5.3.
Their model first converts the sound signal into a
spectrogram, which is a one-dimensional series
, that encodes at every time step a vector
T ×D
of energies in frequency bands. The associ-
D
ated text is encoded with the BPE tokenizer (see
§ 3.2).
The spectrogram is processed through a few
1D convolutional layers, and the resulting rep-
resentation is fed into the encoder of the Trans-
former. The decoder directly generates a discrete
sequence of tokens, that correspond to one of
the possible tasks considered during training.
Multiple objectives are considered: transcription
of English or non-English text, translation from
any language to English, or detection of non-
speech sequences, such as background music or
ambient noise.
129

--- Page 130 ---
This approach allows leveraging extremely large
datasets that combine multiple types of sound
sources with diverse ground truths.
It is noteworthy that even though the ultimate
goal of this approach is to produce a transla-
tion as deterministic as possible given the input
signal, it is formally the sampling of a text dis-
tribution conditioned on a sound sample, hence
a synthesis process. The decoder is, in fact, ex-
tremely similar to the generative model of § 7.1.
130

--- Page 131 ---
6.6 Text-image representations
A powerful approach to image understanding
consists of learning consistent image and text
representations, such that an image, or a textual
description of it, would be mapped to the same
feature vector.
The Contrastive Language-Image Pre-training
(CLIP) proposed by Radford et al. [2021] com-
bines an image encoder , which is a ViT, and
f
a text encoder , which is a GPT. See § 5.3 for
g
both.
TorepurposeaGPTasatextencoder, insteadofa
standard autoregressive model, they add an “end
ofsentence”tokentotheinputsequence, anduse
the representation of this token in the last layer
as the embedding. Its dimension is between
512
and , depending on the configuration.
1024
Those two models are trained from scratch using
a dataset of 400 million image-text pairs
(i ,t )
k k
collected from the internet. The training proce-
dure follows the standard mini-batch stochastic
gradient descent approach but relies on a con-
trastive loss. The embeddings are computed for
every image and every text of the pairs in the
N
mini-batch, and a cosine similarity measure is
computed not only between text and image em-
131

--- Page 132 ---
beddings from each pair, but also across pairs, re-
sulting in an matrix of similarity scores:
N ×N
l = f(i )·g(t ), m = 1,...,N,n = 1,...,N.
m,n m n
The model is trained with cross-entropy so that,
the values interpreted as logit
∀n l ,...,l
1,n N,n
scores predict , and similarly for .
n l ,...,l
n,1 n,N
This means that s.t. the similarity
∀n,m, n ̸= m
isunambiguouslygreaterthanboth and
l l
n,n n,m
.
l
m,n
When it has been trained, this model can be used
to do zero-shot prediction, that is, classifying a
signal in the absence of training examples by
defining a series of candidate classes with text
descriptions, and computing the similarity of the
embedding of an image with the embedding of
each of those descriptions (see Figure 6.4).
Additionally, since the textual descriptions are
often detailed, such a model has to capture a
richer representation of images and pick up cues
beyondwhatisnecessaryforinstanceforclassifi-
cation. This translates to excellent performance
on challenging datasets such as ImageNet Adver-
sarial [Hendrycks et al., 2019] which was specifi-
cally designed to degrade or erase cues on which
standard predictors rely.
132

--- Page 133 ---
Figure 6.4:
The CLIP text-image embedding [Radford
etal.,2021]allowsforzero-shotpredictionbypredicting
which class description embedding is the most consis-
tent with the image embedding.
133

--- Page 134 ---
6.7 Reinforcement learning
Many problems, such as strategy games or
roboticcontrol, canbeformalizedwithadiscrete-
time state process and reward process that
S R
t t
can be modulated by choosing actions . If
A
t
is Markovian, meaning that it carries alone
S
t
as much information about the future as all the
past states until that instant, such an object is a
Markovian Decision Process (MDP).
Given an MDP, the objective is classically to find
a policy such that maximizes the
π A = π(S )
t t
expectation of the return, which is an accumu-
lated discounted reward:
 
(cid:88)
E t
γ R ,
 t
t≥0
for a discount factor .
0 < γ < 1
This is the standard setup of Reinforcement
Learning (RL), and it can be worked out by intro-
ducing the optimal state-action value function
which is the expected return if we exe-
Q(s,a)
cute action in state , and then follow the opti-
a s
mal policy. It provides a means to compute the
optimal policy as , and,
π(s) = argmax Q(s,a)
a
thanks to the Markovian assumption, it verifies
134

--- Page 135 ---
the Bellman equation:
(6.1)
Q(s,a) =
(cid:20) (cid:12) (cid:21)
(cid:12)
E ′
R +γmaxQ(S ,a )(cid:12)S = s,A = a ,
t t+1 t t
a′ (cid:12)
from which we can design a procedure to train
a parametric model .
Q(·,·;w)
To apply this framework to play classical Atari
videogames, Mnihetal.[2015]usefor thecon-
S
t
catenation of the frame at time and the three
t
that precede, so that the Markovian assumption
is reasonable, and use for a model dubbed the
Q
Deep Q-Network (DQN), composed of two con-
volutional layers and one fully connected layer
with one output value per action, following the
classical structure of a LeNet (see § 5.2).
Training is achieved by alternatively playing and
recording episodes, and building mini-batches of
tuples (s ,a ,r ,s′ ) ∼ (S ,A ,R ,S ) taken
n n n n t t t t+1
across stored episodes and time steps, and mini-
mizing
N
1 (cid:88)
2 (6.2)
ℒ(w) = (Q(s ,a ;w)−y )
n n n
N
n=1
with one iteration of SGD, where if this
y = r
n n
tuple is the end of the episode, and
y = r +
n n
γmax Q(s′ ,a;w¯) otherwise.
a n
135

--- Page 136 ---
Framenumber
eulaV
Figure 6.5:
This graph shows the evolution of the state
valueV (S )=max Q(S ,a)duringagameofBreak-
t a t
out. The spikes at time points (1) and (2) correspond to
clearing a brick, at time point (3) it is about to break
through to the top line, and at (4) it does, which ensures
a high future reward [Mnih et al., 2015].
Here is a constant copy of , i.e. the gradient
w¯ w
does not propagate through it to . This is nec-
w
essary since the target value in Equation 6.1 is
the expectation of , while it is itself which
y y
n n
is used in Equation 6.2. Fixing in results in
w y
n
a better approximation of the desirable gradient.
A key issue is the policy used to collect episodes.
Mnih et al. [2015] simply use the -greedy strat-
ϵ
egy, which consists of taking an action com-
pletely at random with probability , and the
ϵ
optimal action otherwise. In-
argmax Q(s,a)
a
jecting a bit of randomness is necessary to favor
136

--- Page 137 ---
exploration.
Training is done with ten million frames corre-
sponding to a bit less than eight days of game-
play. The trained network computes accurate
estimates of the state values (see Figure 6.5), and
reaches human performance on a majority of the
49 games used in the experimental validation.
137
