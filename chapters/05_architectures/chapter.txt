--- Page 98 ---
5
Chapter
Architectures
The field of deep learning has developed over
the years for each application domain multiple
deep architectures that exhibit good trade-offs
with respect to multiple criteria of interest: e.g.
ease of training, accuracy of prediction, memory
footprint, computational cost, scalability.
98

--- Page 99 ---
5.1 Multi-Layer Perceptrons
The simplest deep architecture is the Multi-
Layer Perceptron (MLP), which takes the form
of a succession of fully connected layers sepa-
rated by activation functions. See an example
in Figure 5.1. For historical reasons, in such a
model, the number ofhidden layers refers to the
number of linear layers, excluding the last one.
A key theoretical result is the universal approx-
imation theorem [Cybenko, 1989] which states
that, if the activation function is continuous
σ
Y
2
fully-conn
relu
10
fully-conn
Hidden
layers
relu
25
fully-conn
50
X
Figure 5.1:
This multi-layer perceptron takes as input
a one-dimensional tensor of size 50, is composed of
three fully connected layers with outputs of dimensions
respectively 25, 10, and 2, the two first followed by
ReLU layers.
99

--- Page 100 ---
and not polynomial, any continuous function
f
can be approximated arbitrarily well uniformly
on a compact domain, which is bounded and
contains its boundary, by a model of the form
where and are affine. Such a model
l ◦σ◦l l l
2 1 1 2
is a MLP with a single hidden layer, and this
result implies that it can approximate anything
of practical value. However, this approximation
holds if the dimension of the first linear layer’s
output can be arbitrarily large.
In spite of their simplicity, MLPs remain an im-
portant tool when the dimension of the signal
to be processed is not too large.
100

--- Page 101 ---
5.2 Convolutional networks
The standard architecture forprocessing images
is a convolutional network, or convnet, that
combines multiple convolutional layers, either
to reduce the signal size before it can be pro-
cessed by fully connected layers, or to output a
2D signal also of large size.
LeNet-like
The original LeNet model for image classifica-
tion [LeCun et al., 1998] combines a series of
2Dconvolutional layers andmax pooling layers
that play the role of feature extractor, with a
series of fully connected layers which act as a
MLP and perform the classification per se (see
Figure 5.2).
This architecture was the blueprint for many
models that share its structure and are simply
larger, such as AlexNet [Krizhevsky et al., 2012]
or the VGG family [Simonyan and Zisserman,
2014].
Residual networks
Standard convolutional neural networks that fol-
low the architecture of the LeNet family are not
easily extended to deep architectures and suffer
101

--- Page 102 ---
ˆ
P(Y )
10
fully-conn
Classifier
relu
200
fully-conn
256
reshape
relu
64×2×2
maxpool
k=2
64×4×4
conv-2d
Feature
k=5
extractor
relu
32×8×8
maxpool
k=3
32×24×24
conv-2d
k=5
1×28×28
X
Figure 5.2:
Example of a smallLeNet-like network for
classifying 28×28 grayscale images of handwritten
digits [LeCun et al., 1998]. Its first half is convolutional,
and alternates convolutional layers per se and max
pooling layers, reducing the signal dimension from
28×28 scalars to 256. Its second half processes this
256-dimensional feature vector through a one hidden
layer perceptron to compute 10 logit scores correspond-
ing to the ten possible digits.
102

--- Page 103 ---
Y
C×H×W
relu
+
batchnorm
C×H×W
conv-2d
k=1
relu
batchnorm
conv-2d
k=3p=1
relu
batchnorm
C ×H×W
conv-2d 2
k=1
C×H×W
X
Figure 5.3:
A residual block.
from the vanishing gradient problem. The resid-
ual networks, or ResNets, proposed by He et al.
[2015] explicitly address the issue of the van-
ishing gradient with residual connections (see
§ 4.7), which allow hundreds of layers. They
have become standard architectures for com-
puter vision applications, and exist in multiple
versions depending on the number of layers. We
are going to look in detail at the architecture of
the ResNet-50 for classification.
103

--- Page 104 ---
Y
4C × H × W
relu S S S
+
batchnorm batchnorm
4C × H × W
conv-2d conv-2d S S S
k=1s=S k=1
relu
batchnorm
C × H × W
conv-2d S S S
k=3s=S p=1
relu
batchnorm
C ×H×W
conv-2d S
k=1
C×H×W
X
Figure 5.4:
A downscaling residual block. It admits a
hyper-parameter S, the stride of the first convolution
layer, which modulates the reduction of the tensor size.
As other ResNets, it is composed of a series of
residual blocks, each combining several convo-
lutional layers,batch norm layers, and ReLU lay-
ers, wrapped in a residual connection. Such a
block is pictured in Figure 5.3.
A key requirement for high performance with
real images is to propagate a signal with a large
number of channels, to allow for a rich repre-
104

--- Page 105 ---
ˆ
P(Y )
1000
fully-conn
2048
reshape
2048×1×1
avgpool
k=7
resblock
×2
2048×7×7
dresblock
S=2
resblock
×5
1024×14×14
dresblock
S=2
resblock
×3
512×28×28
dresblock
S=2
resblock
×2
256×56×56
dresblock
S=1
64×56×56
maxpool
k=3s=2p=1
relu
batchnorm
64×112×112
conv-2d
k=7s=2p=3
3×224×224
X
Figure 5.5:
Structure of the ResNet-50 [He et al., 2015].
105

--- Page 106 ---
sentation. However, the parameter count of a
convolutional layer, and its computational cost,
are quadratic with the number of channels. This
residual block mitigates this problem by first re-
ducing the number of channels with a con-
1×1
volution, then operating spatially with a
3×3
convolution on this reduced number of chan-
nels, and then upscaling the number of channels,
again with a convolution.
1×1
The network reduces the dimensionality of the
signal to finally compute the logits for the clas-
sification. This is done thanks to an architec-
ture composed of several sections, each starting
with a downscaling residual block that halves
the height and width of the signal, and doubles
the number of channels, followed by a series
of residual blocks. Such a downscaling resid-
ual block has a structure similar to a standard
residual block, except that it requires a residual
connection that changes the tensor shape. This
is achieved with a convolution with a stride
1×1
of two (see Figure 5.4).
The overall structure of the ResNet-50 is pre-
sented in Figure 5.5. It starts with a convo-
7×7
lutional layer that converts the three-channel in-
put image to a -channel image of half the size,
64
followed by four sections of residual blocks. Sur-
106

--- Page 107 ---
prisingly, in the first section, there is no down-
scaling, only an increase of the number of chan-
nels by a factor of . The output of the last resid-
4
ual block is , which is converted to a
2048×7×7
vector of dimension by an average pooling
2048
of kernel size , and then processed through
7×7
a fully-connected layer to get the final logits,
here for classes.
1000
107

--- Page 108 ---
5.3 Attention models
As stated in § 4.8, many applications, particu-
larly from natural language processing, benefit
greatly from models that include attention mech-
anisms. The architecture of choice for such tasks,
which has been instrumental in recent advances
in deep learning, is the Transformer proposed
by Vaswani et al. [2017].
Transformer
The original Transformer, pictured in Figure 5.7,
was designed for sequence-to-sequence transla-
tion. It combines an encoder that processes the
input sequence to get a refined representation,
and an autoregressive decoder that generates
each token of the result sequence, given the en-
coder’s representation of the input sequence and
the output tokens generated so far.
As the residual convolutional networks of § 5.2,
both the encoder and the decoder of the Trans-
former are sequences of compounded blocks
built with residual connections.
• Thefeed-forward block, pictured at the top of
Figure 5.6 is a one hidden layer MLP, preceded
by alayer normalization. It can update represen-
tations at every position separately.
108

--- Page 109 ---
Y
+
dropout
fully-conn
gelu
fully-conn
layernorm
XQKV
Y Y
+ +
mha mha
Q K V Q K V
layernorm layernorm
XQKV XQ XKV
Figure 5.6:
Feed-forward block (top), self-attention
block (bottom left) and cross-attention block (bottom
right). These specific structures proposed by Radford
et al. [2018] differ slightly from the original architec-
ture of Vaswani et al. [2017], in particular by having
the layer normalization first in the residual blocks.
109

--- Page 110 ---
ˆ ˆ
P(Y ),...,P(Y |Y )
1 S s<S
S×V
fully-conn
S×D
ffw
cross-att
Decoder Q KV
causal
self-att
×N
pos-enc +
S×D
embed
S
0,Y ,...,Y
1 S−1
Z ,...,Z
1 T
T×D
ffw
self-att
Encoder ×N
pos-enc +
T×D
embed
T
X ,...,X
1 T
Figure 5.7:
Original encoder-decoder Transformer
model for sequence-to-sequence translation [Vaswani
et al., 2017].
110

--- Page 111 ---
• The self-attention block, pictured on the bot-
tom left of Figure 5.6, is a Multi-Head Attention
layer (see § 4.8), that recombines information
globally, allowing any position to collect infor-
mation from any other positions, preceded by
a layer normalization. This block can be made
causal by using an adequate mask in the atten-
tion layer, as described in § 4.8
• Thecross-attention block, pictured on the bot-
tom right of Figure 5.6, is similar except that it
takes as input two sequences, one to compute
the queries and one to compute the keys and
values.
The encoder of the Transformer (see Figure
5.7, bottom), recodes the input sequence of dis-
crete tokens with anembedding layer
X ,...X
1 T
(see § 4.9), and adds a positional encoding (see
§ 4.10), before processing it with several self-
attention blocks to generate a refined represen-
tation .
Z ,...,Z
1 T
The decoder (see Figure 5.7, top), takes as in-
put the sequence of result tokens
Y ,...,Y
1 S−1
produced so far, similarly recodes them through
an embedding layer, adds a positional encoding,
and processes it through alternating causal self-
attention blocks and cross-attention blocks to
111

--- Page 112 ---
ˆ ˆ
P(X ),...,P(X |X )
1 T t<T
T×V
fully-conn
T×D
ffw
causal
self-att
×N
pos-enc +
T×D
embed
T
0,X ,...,X
1 T−1
Figure 5.8:
GPT model [Radford et al., 2018].
produce the logits predicting the next tokens.
These cross-attention blocks compute their keys
and values from the encoder’s result represen-
tation , which allows the resulting se-
Z ,...,Z
1 T
quence to be a function of the original sequence
.
X ,...,X
1 T
As we saw in § 3.2 being causal ensures that
such a model can be trained by minimizing the
cross-entropy summed across the full sequence.
Generative Pre-trained Transformer
The Generative Pre-trained Transformer (GPT)
[Radford et al., 2018, 2019], pictured in Figure 5.8
112

--- Page 113 ---
is a pure autoregressive model that consists of a
succession of causal self-attention blocks, hence
a causal version of the original Transformer en-
coder.
This class of models scales extremely well, up
to hundreds of billions of trainable parameters
[Brown et al., 2020]. We will come back to their
use for text generation in § 7.1.
Vision Transformer
Transformers have been put to use for image
classification with the Vision Transformer (ViT)
model [Dosovitskiy et al., 2020] (see Figure 5.9).
It splits the three-channel input image into
M
patches of resolution , which are then flat-
P ×P
tenedtocreateasequenceofvectors
X ,...,X
1 M
of shape
M
×3P2. This sequence is multiplied
by a trainable matrix WE of shape 3P2×D to
map it to an sequence, to which is con-
M ×D
catenated one trainable vector . The resulting
E
0
sequence is then pro-
(M +1)×D E ,...,E
0 M
cessed through multiple self-attention blocks.
See § 5.3 and Figure 5.6.
The first element in the resultant sequence,
Z
0
which corresponds to and is not associated
E
0
with any part of the image, is finally processed
113

--- Page 114 ---
ˆ
P(Y )
C
fully-conn
gelu
MLP
fully-conn
readout
gelu
fully-conn
D
Z ,Z ,...,Z
0 1 M
(M+1)×D
ffw
self-att
×N
pos-enc +
(M+1)×D
E ,E ,...,E
0 1 M
E ×WE
Image 0
encoder
M×3P2
X ,...,X
1 M
Figure 5.9:
Vision Transformer model [Dosovitskiy
et al., 2020].
114

--- Page 115 ---
by a two-hidden-layer MLP to get the final
C
logits. Such a token, added for a readout of a
class prediction, was introduced by Devlin et al.
[2018] in the BERT model and is referred to as a
CLS token.
115

--- Page 116 ---
Part III
Applications
116
