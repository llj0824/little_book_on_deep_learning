--- Page 11 ---
1
Chapter
Machine Learning
Deep learning belongs historically to the larger
field of statistical machine learning, as it funda-
mentally concerns methods that are able to learn
representations from data. The techniques in-
volved come originally fromartificial neural net-
works, and the ‚Äúdeep‚Äù qualifier highlights that
models are long compositions of mappings, now
known to achieve greater performance.
The modularity, versatility, and scalability of
deep models have resulted in a plethora of spe-
cific mathematical methods and software devel-
opment tools, establishing deep learning as a
distinct and vast technical field.
11

--- Page 12 ---
1.1 Learning from data
The simplest use case for a model trained from
data is when a signal is accessible, for instance,
x
the picture of a license plate, from which one
wants to predict a quantity , such as the string
y
of characters written on the plate.
In many real-world situations where is a high-
x
dimensional signal captured in an uncontrolled
environment, it is too complicated to come up
with an analytical recipe that relates and .
x y
What one can do is to collect a large training
set of pairs , and devise a paramet-
ùíü (x ,y )
n n
ric model . This is a piece of computer code
f
that incorporates trainable parameters that
w
modulate its behavior, and such that, with the
proper values w‚àó, it is a good predictor. ‚ÄúGood‚Äù
here means that if an is given to this piece
x
of code, the value yÀÜ= f(x;w‚àó) it computes is
a good estimate of the that would have been
y
associated with in the training set had it been
x
there.
This notion of goodness is usually formalized
with a loss which is small when
‚Ñí(w) f(¬∑;w)
is good on . Then, training the model consists
ùíü
of computing a value w‚àó that minimizes ‚Ñí(w‚àó) .
12

--- Page 13 ---
Most of the content of this book is about the defi-
nition of , which, in realistic scenarios, is a com-
f
plex combination of pre-defined sub-modules.
The trainable parameters that compose are of-
w
ten calledweights, by analogy with the synaptic
weights of biological neural networks. In addi-
tion to these parameters, models usually depend
on hyper-parameters, which are set according
to domain prior knowledge, best practices, or re-
source constraints. They may also be optimized
in some way, but with techniques different from
those used to optimize .
w
13

--- Page 14 ---
1.2 Basis function regression
We can illustrate the training of a model in a sim-
ple case where and are two real numbers,
x y
n n
the loss is the mean squared error:
N
1 (cid:88)
2 (1.1)
‚Ñí(w) = (y ‚àíf(x ;w)) ,
n n
N
n=1
and is a linear combination of a pre-
f(¬∑;w)
defined basis of functions , with
f ,...,f w =
1 K
:
(w ,...,w )
1 K
K
(cid:88)
f(x;w) = w f (x).
k k
k=1
Since is linear with respect to the s
f(x ;w) w
n k
and is quadratic with respect to ,
‚Ñí(w) f(x ;w)
n
Figure 1.1:
Given a basis of functions (blue curves)
and a training set (black dots), we can compute an
optimal linear combination of the former (red curve)
to approximate the latter for the mean squared error.
14

--- Page 15 ---
the loss is quadratic with respect to the
‚Ñí(w)
w s, and finding w‚àó that minimizes it boils down
k
to solving a linear system. See Figure 1.1 for an
example with Gaussian kernels as .
f
k
15

--- Page 16 ---
1.3 Under and overfitting
A key element is the interplay between the ca-
pacity of the model, that is its flexibility and
ability to fit diverse data, and the amount and
quality of the training data. When the capacity
is insufficient, the model cannot fit the data, re-
sulting in a high error during training. This is
referred to as underfitting.
On the contrary, when the amount of data is in-
sufficient, as illustrated in Figure 1.2, the model
will often learn characteristics specific to the
training examples, resulting in excellent perfor-
mance during training, at the cost of a worse
Figure 1.2:
If the amount of training data (black dots)
is small compared to the capacity of the model, the em-
pirical performance of the fitted model during training
(red curve) reflects poorly its actual fit to the underly-
ing data structure (thin black curve), and consequently
its usefulness for prediction.
16

--- Page 17 ---
fit to the global structure of the data, and poor
performance on new inputs. This phenomenon
is referred to as overfitting.
So, a large part of the art of applied machine
learning is to design models that are not too
flexible yet still able to fit the data. This is done
by crafting the right inductive bias in a model,
which means that its structure corresponds to
the underlying structure of the data at hand.
Even though this classical perspective is relevant
for reasonably-sized deep models, things get con-
fusing with large ones that have a very large
number of trainable parameters and extreme ca-
pacity yet still perform well on prediction. We
will come back to this in ¬ß 3.6 and ¬ß 3.7.
17

--- Page 18 ---
1.4 Categories of models
We can organize the use of machine learning
models into three broad categories:
‚Ä¢ Regression consists of predicting a
continuous-valued vector
y ‚àà
RK, for instance,
a geometrical position of an object, given an
input signal . This is a multi-dimensional
X
generalization of the setup we saw in ¬ß 1.2. The
training set is composed of pairs of an input
signal and a ground-truth value.
‚Ä¢ Classification aims at predicting a value from
a finite set , for instance, the label of
{1,...,C} Y
an image . As with regression, the training set
X
is composed of pairs of input signal, and ground-
truth quantity, here a label from that set. The
standard way of tackling this is to predict one
score per potential class, such that the correct
class has the maximum score.
‚Ä¢ Densitymodeling hasasitsobjectivetomodel
the probability density function of the data
¬µ
X
itself, for instance, images. In that case, the train-
ing set is composed of values without associ-
x
n
ated quantities to predict, and the trained model
should allow for the evaluation of the probability
density function, or sampling from the distribu-
tion, or both.
18

--- Page 19 ---
Both regression and classification are gener-
ally referred to as supervised learning, since the
value to be predicted, which is required as a tar-
get during training, has to be provided, for in-
stance, by human experts. On the contrary, den-
sity modeling is usually seen as unsupervised
learning, since it is sufficient to take existing
data without the need for producing an associ-
ated ground-truth.
These three categories are not disjoint; for in-
stance, classification can be cast as class-score
regression, or discrete sequence density model-
ing as iterated classification. Furthermore, they
do not cover all cases. One may want to predict
compounded quantities, or multiple classes, or
model a density conditional on a signal.
19
