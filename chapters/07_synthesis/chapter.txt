--- Page 138 ---
7
Chapter
Synthesis
A second category of applications distinct from
prediction is synthesis. It consists of fitting a
density model to training samples and providing
means to sample from this model.
138

--- Page 139 ---
7.1 Text generation
The standard approach to text synthesis is to
use an attention-based, autoregressive model. A
very successful model proposed by Radford et al.
[2018], is the GPT which we described in ¬ß 5.3.
This architecture has been used for very large
models, such as OpenAI‚Äôs 175-billion-parameter
GPT-3 [Brown et al., 2020]. It is composed of 96
self-attention blocks, each with 96 heads, and
processes tokens of dimension 12,288, with a
hidden dimension of 49,512 in the MLPs of the
attention blocks.
When such a model is trained on a very large
dataset, it results in a Large Language Model
(LLM), which exhibits extremely powerful prop-
erties. Besides the syntactic and grammatical
structure of the language, it has to integrate
very diverse knowledge, e.g. to predict the word
following ‚ÄúThe capital of Japan is‚Äù, ‚Äúif water is
heated to 100 Celsius degrees it turns into‚Äù, or
‚Äúbecause her puppy was sick, Jane was‚Äù.
This results in particular in the ability to solve
few-shot prediction, where only a handful of
training examples are available, as illustrated
in Figure 7.1. More surprisingly, when given
a carefully crafted prompt, it can exhibit abil-
139

--- Page 140 ---
I: I love apples, O: positive, I: music is my passion, O:
positive, I: my job is boring, O: negative, I: frozen pizzas
are awesome, O:
positive,
I: I love apples, O: positive, I: music is my passion, O:
positive, I: my job is boring, O: negative, I: frozen pizzas
taste like cardboard, O:
negative,
I: water boils at 100 degrees, O: physics, I: the square
root of two is irrational, O: mathematics, I: the set of
prime numbers is infinite, O: mathematics, I: gravity is
proportional to the mass, O:
physics,
I: water boils at 100 degrees, O: physics, I: the square
root of two is irrational, O: mathematics, I: the set of
prime numbers is infinite, O: mathematics, I: squares
are rectangles, O:
mathematics,
Figure 7.1:
Examples of few-shot prediction with a 120
million parameter GPT model from Hugging Face. In
each example, the beginning of the sentence was given
as aprompt, and the model generated the part in bold.
ities for question answering, problem solving,
and chain-of-thought that appear eerily close to
high-level reasoning [Chowdhery et al., 2022;
Bubeck et al., 2023].
Due to these remarkable capabilities, these mod-
els are sometimes called foundation models
[Bommasani et al., 2021].
However, even though it integrates a very large
body of knowledge, such a model may be inad-
140

--- Page 141 ---
equate for practical applications, in particular
when interacting with human users. In many
situations, one needs responses that follow the
statistics of a helpful dialog with an assistant.
This differs from the statistics of available large
training sets, which combine novels, encyclope-
dias, forum messages, and blog posts.
This discrepancy is addressed by fine-tuning
such a language model (see ¬ß 3.6). The current
dominant strategy is Reinforcement Learning
from Human Feedback (RLHF) [Ouyang et al.,
2022], which consists of creating small labeled
training sets by asking users to either write
responses or provide ratings of generated re-
sponses. The former can be used as-is to fine-
tune the language model, and the latter can be
used to train a reward network that predicts
the rating and use it as a target to fine-tune the
language model with a standard Reinforcement
Learning approach.
141

--- Page 142 ---
7.2 Image generation
Multiple deep methods have been developed to
model and sample from a high-dimensional den-
sity. A powerful approach for image synthesis
relies on inverting a diffusion process. Such a
generative model is referred to, somehow incor-
rectly, as a diffusion model.
The principle consists of defining analytically
a process that gradually degrades any sample,
and consequently transforms the complex and
unknown density of the data into a simple and
well-known density such as a normal, and train-
ing a deep architecture to invert this degradation
process [Ho et al., 2020].
Given a fixed , the diffusion process defines a
T
probability distribution over series of im-
T +1
ages as follows: sample uniformly from the
x
0
dataset, and then sequentially sample
x ‚àº
t+1
, where the condi-
p(x | x ),t = 0,...,T ‚àí1
t+1 t
tional distribution is defined analytically and
p
such that it gradually erases the structure that
was in . The setup should degrade the signal
x
0
so much that the distribution has a known
p(x )
T
analytical form which can be sampled.
For instance, Ho et al. [2020] normalize the data
to have a mean of and a variance of , and their
0 1
142

--- Page 143 ---
x
T
x
0
Figure 7.2:
Image synthesis with denoising diffusion
[Ho et al., 2020]. Each sample starts as a white noise
x (top), and is gradually de-noised by sampling iter-
T
atively x |x ‚àºùí© (x +f(x ,t;w),œÉ ).
t‚àí1 t t t t
143

--- Page 144 ---
diffusion process consists of adding a bit of white
noise and re-normalizing the variance to . This
1
process exponentially reduces the importance of
, and ‚Äôs density can rapidly be approximated
x x
0 t
with a normal.
The denoiser is a deep architecture that
f
should model and allow sampling from
. It can be shown,
f(x ,x ,t;w) ‚âÉ p(x | x )
t‚àí1 t t‚àí1 t
thanks to a variational bound, that if this
one-step reverse process is accurate enough,
sampling and denoising steps
x ‚àº p(x ) T
T T
with results in that follows .
f x p(x )
0 0
Training can be achieved by generating a large
f
number of sequences (n) (n), picking a
x ,...,x t
0 T n
in each, and maximizing
(cid:88) (cid:16) (cid:17)
(n) (n)
logf x ,x ,t ;w .
t ‚àí1 t n
n n
n
Given their diffusion process, Ho et al. [2020]
have a denoising of the form:
(7.1)
x | x ‚àº ùí© (x +f(x ,t;w);œÉ ),
t‚àí1 t t t t
where is defined analytically.
œÉ
t
In practice, such a model initially hallucinates
structures by pure luck in the random noise, and
144

--- Page 145 ---
thengraduallybuildsmoreelementsthatemerge
from the noise by reinforcing the most likely
continuation of the image obtained thus far.
This approach can be extended to text-
conditioned synthesis, to generate images
that match a description. For instance, Nichol
et al. [2021] add to the mean of the denoising
distribution of Equation 7.1 a bias that goes in
the direction of increasing the CLIP matching
score (see ¬ß 6.6) between the produced image
and the conditioning text description.
145
