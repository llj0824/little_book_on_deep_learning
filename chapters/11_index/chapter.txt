--- Page 176 ---
Index
1D convolution, 66
2D convolution, 66
activation, 23, 41
function, 71, 99
map, 69
Adam, 39, 154
adapter, 153
affine operation, 61
artificial neural network, 8, 11
attention operator, 88
autoencoder, 159
denoising, 118
Autograd, 42
autoregressive model, model, autoregressive
see
average pooling, 76
backpropagation, 42
backward pass, 42, 154
basis function regression, 14
batch, 21, 38
batch normalization, 80, 104
176

--- Page 177 ---
Bellman equation, 135
bias vector, 61, 67
BPE, Byte Pair Encoding
see
Byte Pair Encoding, 34, 129
cache memory, 21
capacity, 16
causal, 32, 90, 111
model, model, causal
see
chain rule (derivative), 40
chain rule (probability), 30
chain-of-thought, 140, 149
channel, 23
checkpointing, 43
classification, 18, 26, 101, 120
CLIP, Contrastive Language-Image
see
Pre-training
CLS token, 115
computational cost, 43, 91
context size, 147
Contrastive Language-Image Pre-training, 131,
156
contrastive loss, 27, 131
convnet, convolutional network
see
convolution, 66
convolutional layer, layer, convolutional
see
convolutional network, 101
cross-attention block, 94, 109, 111
cross-entropy, 27, 31, 45
177

--- Page 178 ---
data augmentation, 120
deep learning, 8, 11
Deep Q-Network, 135
denoising autoencoder, autoencoder,
see
denoising
density modeling, 18
depth, 41
diffusion model, 142
dilation, 67, 74
discriminator, 160
downscaling residual block, 106
downstream task, 50
DQN, Deep Q-Network
see
dropout, 77, 91
embedding layer, layer, embedding
see
epoch, 48
equivariance, 67, 94
feed-forward block, 108, 109
few-shot prediction, 139
filter, 66
fine-tune, 124
fine-tuning, 51, 141
flops, 22
forward pass, 41
foundation model, 140
FP32, 22
framework, 23
178

--- Page 179 ---
GAN, Generative Adversarial Networks
see
GELU, 73
Generative Adversarial Networks, 160
Generative Pre-trained Transformer, 112, 131,
139, 162
generator, 160
GNN, Graph Neural Network
see
GPT, Generative Pre-trained Transformer
see
GPU, Graphical Processing Unit
see
gradient descent, 35, 37, 40, 45
gradient norm clipping, 44
gradient step, 35
Graph Neural Network, 161
Graphical Processing Unit, 8, 20
ground truth, 18
hidden layer, layer, hidden
see
hidden state, 158
hyper parameter, parameter, hyper
see
hyperbolic tangent, 72
image processing, 101
image synthesis, 87, 142
inductive bias, 17, 49, 66, 67, 96
invariance, 76, 94, 96, 162
kernel size, 66, 74
key, 88
Large Language Model, 51, 56, 88, 139, 146, 162
179

--- Page 180 ---
layer, 41, 59
attention, 87
convolutional, 66, 74, 87, 96, 101, 104, 121,
126, 129
embedding, 95, 111
fully connected, 61, 87, 96, 99, 101
hidden, 99
linear, 61
Multi-Head Attention, 93, 96, 111
normalizing, 80
reversible, 43
layer normalization, 83, 108, 111
Leaky ReLU, 72
learning rate, 35, 50
learning rate schedule, 50
LeNet, 101, 102
linear layer, layer, linear
see
LLM, Large Language Model
see
local minimum, 35
logit, 26, 31
LoRA, Low-Rank Adaptation
see
loss, 12
Low-Rank Adaptation, 153, 155
machine learning, 11, 17, 18
Markovian Decision Process, 134
Markovian property, 134
max pooling, 74, 101
MDP, Markovian, Decision Process
see
180

--- Page 181 ---
mean squared error, 14, 26
memory requirement, 43
memory speed, 21
metric learning, 27
MLP, multi-layer perceptron, 154
see
model, 12
autoregressive, 30, 31, 139
causal, 33, 91, 111, 112
parametric, 12
pre-trained, 51, 124, 128
model merging, 156
multi-layer perceptron, 45, 99–101, 108
Natural Language Processing, 87
NLP, Natural Language Processing
see
non-linearity, 71
normalizing layer, layer, normalizing
see
object detection, 121
overfitting, 17, 48
padding, 67, 74
parameter, 12
hyper, 13, 35, 48, 66, 67, 74, 93, 95
parametric model, model, parametric
see
peak performance, 22
Perplexity, 151
perplexity, 31
policy, 134
optimal, 134
181

--- Page 182 ---
pooling, 74
positional encoding, 96, 111
Post-Training Quantization, 150
posterior probability, 26
pre-trained model, model, pre-trained
see
prompt, 139, 140
engineering, 147
quantization, 150
Quantization-Aware Training, 152
query, 88
RAG, Retrieval-Augmented Generation
see
random initialization, 62
receptive field, 68, 69, 124
rectified linear unit, 71, 158
recurrent neural network, 158
regression, 18
Reinforcement Learning, 134, 141
Reinforcement Learning from Human Feedback,
141
ReLU, rectified linear unit
see
residual
block, 104
connection, 84, 103
network, 47, 84, 103
ResNet-50, 103
Retrieval-Augmented Generation, 149
return, 134
182

--- Page 183 ---
reversible layer, layer, reversible
see
RL, Reinforcement Learning
see
RLHF, Reinforcement Learning from Human
see
Feeback
RNN, recurrent neural network
see
scaling laws, 52
self-attention block, 94, 109, 111
self-supervised learning, 162
semantic segmentation, 86, 126
SGD, stochastic gradient descent
see
Single Shot Detector, 121
skip connection, 84, 127, 158
softargmax, 26, 89
softmax, 26
speech recognition, 129
SSD, Single Shot Detector
see
stochastic gradient descent, 38, 45, 52
stride, 67, 74
supervised learning, 19
Tanh, hyperbolic tangent
see
Task Arithmetic, 156
tensor, 23
tensor cores, 21
Tensor Processing Unit, 21
test set, 48
text synthesis, 139
token, 30
183

--- Page 184 ---
tokenizer, 34, 129
TPU, Tensor Processing Unit
see
trainable parameter, 12, 23, 52
training, 12
training set, 12, 25, 48
Transformer, 47, 84, 88, 96, 108, 110, 129
transformer, 154
transposed convolution, 69, 126
underfitting, 16
universal approximation theorem, 99
unsupervised learning, 19
VAE, variational, autoencoder
see
validation set, 48
value, 88
vanishing gradient, 44, 58
variational
autoencoder, 160
bound, 144
Vision Transformer, 113, 131
ViT, Vision Transformer
see
vocabulary, 30
weight, 13
decay, 28
matrix, 61
zero-shot prediction, 132
184

--- Page 185 ---
This book is licensed under the Creative Com-
mons BY-NC-SA 4.0 International License.
V1.2–May 19, 2024
185
